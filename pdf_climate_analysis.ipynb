{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ8SIPngcCUlp3AzIzkOQ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolinnamaria/PDF-Climate-Analysis/blob/main/pdf_climate_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aRzDMnewd08"
      },
      "outputs": [],
      "source": [
        "# Install the libraries NLKT and stopwords download\n",
        "\n",
        "!pip install PyPDF2 --upgrade\n",
        "!pip install pdfminer.six\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Import Google Drive - where the folders are\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setlist of excluded words: feed this list according to the results obtained in the word cloud\n",
        "#that may hinder the interpretation of the result, such as specific words, prepositions, adverbs, etc\n",
        "excluded_words = set(['word1', 'word2', 'word3'])\n",
        "prepositions = set(stopwords.words('portuguese'))  # prepositions in the language of the PDF\n",
        "adverbs = set(stopwords.words('portuguese')) - {'não'}  # adverbs (excluding the word \"no\" in the language of the PDF)\n",
        "\n",
        "#Select the key-word. In this case the key-word was \"climate change\" in portuguese and some variables of the expression\n",
        "target_words = [\"mudança do clima\", \"mudanças climáticas\", \"mudança climática\", \"mudança global do clima\"]\n",
        "\n",
        "# Function to get words before and after the search word, excluding prepositions, adverbs and specific words\n",
        "def get_context_words(file_paths, target_words):\n",
        "    context_words = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_reader = PdfReader(file)\n",
        "            num_pages = len(pdf_reader.pages)\n",
        "\n",
        "            for page_num in range(num_pages):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "\n",
        "                # Tokenization\n",
        "                tokens = word_tokenize(text.lower())\n",
        "\n",
        "                # Find indexes of the words searched\n",
        "                indexes = [i for i, token in enumerate(tokens) if token in target_words]\n",
        "\n",
        "                # Exclude prepositions, adverbs and quoted words from context words\n",
        "                for index in indexes:\n",
        "                    if index >= 7 and index + 8 <= len(tokens):\n",
        "                        context_before = [word for word in tokens[index - 7:index] if word.isalnum() and word not in prepositions and word not in adverbs and word not in excluded_words]\n",
        "                        context_after = [word for word in tokens[index + 1:index + 8] if word.isalnum() and word not in prepositions and word not in adverbs and word not in excluded_words]\n",
        "                        context_words.extend(context_before + context_after)\n",
        "\n",
        "    return context_words\n",
        "\n",
        "# Function to create word clouds with variable size and black color\n",
        "def generate_wordcloud(text, year):\n",
        "    # Tokenization and removal of stopwords, prepositions, adverbs and specific words\n",
        "    stop_words = set(stopwords.words('portuguese')) | prepositions | adverbs | excluded_words\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words and word not in excluded_words]\n",
        "\n",
        "    # If there are no words after filtering, do not create the word cloud\n",
        "    if not filtered_tokens:\n",
        "        print(f\"No words to create the word cloud for the year {year}.\")\n",
        "        return\n",
        "\n",
        "    # Creation of the word cloud with variable size and black color\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='gray', color_func=lambda *args, **kwargs: 'black').generate(filtered_text)\n",
        "\n",
        "    # Word cloud display\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f\"Nuvem de Palavras - Ano {year}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Folder in Google Drive: write the path of the folder with the PDFs in google drive\n",
        "folder_path = \"/content/drive/MyDrive/Path_of_your_folder_with_the_PDFs\"\n",
        "\n",
        "# Dictionary to store PDF file paths for each year\n",
        "file_paths_by_year = {}\n",
        "\n",
        "# Processing the PDF files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.pdf'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # In the case of the texts I used, the year was in the title of each PDF.\n",
        "        # Here's how to extract the year from the title using a regular expression so that you can analyze the documents by year\n",
        "        match = re.search(r'\\b(20\\d{2})\\b', file_name)\n",
        "        if match:\n",
        "            year = int(match.group())\n",
        "\n",
        "            # Adding the file path to the dictionary corresponding to the year\n",
        "            if year not in file_paths_by_year:\n",
        "                file_paths_by_year[year] = []\n",
        "            file_paths_by_year[year].append(file_path)\n",
        "\n",
        "# Creating word clouds for each year\n",
        "for year, file_paths in file_paths_by_year.items():\n",
        "    print(f\"Analyzing documents for the year {year}:\")\n",
        "    context_words = get_context_words(file_paths, target_words)\n",
        "    print(f'Context words {target_words} - year {year}:', context_words)\n",
        "\n",
        "    # Creating a word cloud for the year\n",
        "    all_text = ' '.join([page.extract_text() for file_path in file_paths for page in PdfReader(file_path).pages])\n",
        "    generate_wordcloud(all_text, year)"
      ]
    }
  ]
}